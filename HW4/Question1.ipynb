{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NSFrozenLake\n",
    "import numpy as np\n",
    "from amalearn.agent import AgentBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSFrozenLake_new(NSFrozenLake):\n",
    "    def __init__(self, studentNum: int = 256, nonStationary=False):\n",
    "        super().__init__(studentNum, nonStationary)\n",
    "        self.actions = {0:(0,-1), 1:(1,0), 2:(0,1), 3:(-1,0)}\n",
    "        self.states = np.array([[(i,j) for j in range (0,4)] for i in range(0,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class valueIterationAgent(AgentBase):\n",
    "\n",
    "    def __init__(self, env, id, discount, theta):\n",
    "        super(valueIterationAgent, self).__init__(id, env)\n",
    "        self.gamma = discount\n",
    "        self.theta = theta\n",
    "        self.values = np.zeros((4,4))\n",
    "        # print(\"values shape: \", self.values.shape)\n",
    "        self.q_table = np.zeros((4,4,4))\n",
    "        self.policy = np.random.randint(0, 4, size=(4,4))\n",
    "\n",
    "    \n",
    "    def calc_qsa(self, action, state_now):\n",
    "        final_reward = 0\n",
    "        # print(state_now)\n",
    "        next_states, probs, fail_probs, dones = self.environment.possible_consequences(action, tuple(state_now))\n",
    "        \n",
    "        for i in range(len(next_states)):\n",
    "            final_reward += probs[i] * (fail_probs[i]*(-10) + (-1) + (dones[i] * 50) + self.gamma*self.values[tuple(next_states[i])])\n",
    "\n",
    "        return final_reward\n",
    "\n",
    "    def gather_qvalues(self, state):\n",
    "        q_values_s = np.array([])\n",
    "        for action in list(self.environment.actions.keys()):\n",
    "            qsa = self.calc_qsa(action, state)\n",
    "            self.q_table[action][tuple(state)] = qsa\n",
    "            q_values_s = np.append(q_values_s, qsa)\n",
    "        return q_values_s \n",
    "    \n",
    "    def value_iteration(self):\n",
    "        steps = 0\n",
    "        while(True):\n",
    "            delta = 0\n",
    "            for row in range(len(self.environment.states)):\n",
    "                for s in self.environment.states[row]:\n",
    "                    # print(\"state: \", s)\n",
    "                    temp_value_s = self.values[tuple(s)]\n",
    "                    q_values_s = self.gather_qvalues(s)\n",
    "                    # print(\"max_qvalues_s: \", np.max(q_values_s))\n",
    "                    # print(tuple(s))\n",
    "                    self.values[tuple(s)] = np.max(q_values_s)\n",
    "                    # print(\"values[tuple(s)]: \", self.values[tuple(s)])\n",
    "\n",
    "                    delta = max(delta, abs(temp_value_s - self.values[tuple(s)]))\n",
    "            steps += 1\n",
    "            if delta < self.theta:\n",
    "                # print(\"okay that's enough!\")\n",
    "                break\n",
    "\n",
    "        for row in range(len(self.environment.states)):\n",
    "            for s in self.environment.states[row]:\n",
    "                self.policy[tuple(s)] = np.argmax(self.gather_qvalues(s)) \n",
    "        \n",
    "        return self.policy, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_NUM = 810196662\n",
    "env = NSFrozenLake_new(STUDENT_NUM)\n",
    "state = env.reset()\n",
    "agent = valueIterationAgent(env, 0, 0.9, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy: \n",
      " [[1 1 1 1]\n",
      " [2 1 1 1]\n",
      " [2 2 1 1]\n",
      " [2 2 2 1]]\n",
      "final q_values: \n",
      " [[[225.55948707 227.64942052 253.97105688 279.05107082]\n",
      "  [252.81168329 255.46652121 287.57242763 322.72949219]\n",
      "  [277.81026012 280.84092442 325.02396053 365.5663774 ]\n",
      "  [304.3107108  308.02773237 355.93695765 411.05017888]]\n",
      "\n",
      " [[250.65460557 281.56842809 316.90758012 348.03495889]\n",
      "  [274.90616454 317.21003296 357.8536099  400.39791549]\n",
      "  [302.24847875 348.19760894 403.9768947  455.29978427]\n",
      "  [304.3107108  350.94619827 407.1822378  459.19221116]]\n",
      "\n",
      " [[249.01329661 274.06313565 301.28768743 303.40779579]\n",
      "  [281.63557236 316.7617355  349.05102326 351.7367626 ]\n",
      "  [317.45929062 357.8515145  402.24408637 405.42494365]\n",
      "  [347.2291767  402.19147841 455.32427009 459.19221116]]\n",
      "\n",
      " [[225.55948707 251.10323006 276.93096246 303.40779579]\n",
      "  [227.71656478 253.82521225 280.06713519 307.1095995 ]\n",
      "  [255.71577887 287.57033224 324.57566307 356.76379076]\n",
      "  [279.87249218 323.23854424 365.59086322 409.31737054]]]\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "final_policy, steps = agent.value_iteration()\n",
    "print(\"final policy: \\n\", final_policy)\n",
    "print(\"final q_values: \\n\", agent.q_table)\n",
    "print(steps)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
