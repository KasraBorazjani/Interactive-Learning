{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kasra Borazjani - 810196662\n",
        "\n",
        "## Interactive Learning - HW3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBMoPLmGbrIn"
      },
      "outputs": [],
      "source": [
        "from amalearn.reward import RewardBase\n",
        "from amalearn.agent import AgentBase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBACGmh0brIr"
      },
      "outputs": [],
      "source": [
        "from amalearn.environment import EnvironmentBase\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH6sNHxPbrIs"
      },
      "outputs": [],
      "source": [
        "class Environment(EnvironmentBase):\n",
        "    def __init__(self, obstacle = [] ,id = 0, action_count=9, actionPrice = -1, goalReward = 100, punish=-10, j_limit = 10, i_limit = 10, p = 0.8, container=None):\n",
        "        \"\"\"\n",
        "        initialize your variables\n",
        "        \"\"\"\n",
        "\n",
        "        self.obstacles = obstacle\n",
        "        self.action_count = action_count\n",
        "        self.id = id\n",
        "        self.action_price = actionPrice\n",
        "        self.goal_reward = goalReward\n",
        "        self.obstacle_punishment = punish\n",
        "        self.x_lim = i_limit\n",
        "        self.y_lim = j_limit\n",
        "        self.p = p\n",
        "\n",
        "        \n",
        "    def isStatePossible(self, state):\n",
        "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
        "        if (state[0]>=0 and state[1]>=0 and state[0]<self.x_lim and state[1]<self.y_lim and (state[0], state[1]) not in self.obstacles):\n",
        "            return True\n",
        "        \n",
        "        else:\n",
        "            return False\n",
        "        \n",
        "    \n",
        "    def isAccessible(self, state, state_p):\n",
        "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
        "        possible_states = [(state[0]+i, state[1]+j) for i in range(-1,2)]\n",
        "\n",
        "    \n",
        "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
        "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
        "    \n",
        "    def getReward(self, state, action, state_p):\n",
        "        \"\"\"return reward of transition\"\"\"\n",
        "    \n",
        "    def sample_all_rewards(self):\n",
        "        return \n",
        "    \n",
        "    def calculate_reward(self, action):\n",
        "        return \n",
        "\n",
        "    def terminated(self):\n",
        "        return \n",
        "\n",
        "    def observe(self):\n",
        "        return \n",
        "\n",
        "    def available_actions(self):\n",
        "        return \n",
        "\n",
        "    def next_state(self, action):\n",
        "        return\n",
        "\n",
        "    def reset(self):\n",
        "        return\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        #print('{}:\\taction={}'.format(self.state['length'], self.state['last_action']))\n",
        "        return \n",
        "\n",
        "    def close(self):\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "898Jlhsycyes"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent(AgentBase):\n",
        "    def __init__(self, id, environment, discount, theta):\n",
        "        #initialize a random policy and V(s) = 0 for each state\n",
        "        self.environment = environment\n",
        "        self.mapp = {}\n",
        "        #mapp states to its ids\n",
        "        self.V = {}\n",
        "        #init V\n",
        "        self.policy = {}\n",
        "        #init policy\n",
        "        super(Agent, self).__init__(id, environment)\n",
        "        self.discount = discount\n",
        "        self.theta = theta\n",
        "        \n",
        "    def policy_evaluation(self):\n",
        "        pass\n",
        "    def policy_improvement(self):\n",
        "        pass\n",
        "    \n",
        "    def value_iteration(self):\n",
        "        pass\n",
        "    \n",
        "    def take_action(self) -> (object, float, bool, object):\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "env.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
