{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kasra Borazjani - 810196662\n",
        "\n",
        "## Interactive Learning - HW3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mBMoPLmGbrIn"
      },
      "outputs": [],
      "source": [
        "from amalearn.reward import RewardBase\n",
        "from amalearn.agent import AgentBase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YBACGmh0brIr"
      },
      "outputs": [],
      "source": [
        "from amalearn.environment import EnvironmentBase\n",
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pH6sNHxPbrIs"
      },
      "outputs": [],
      "source": [
        "class Environment(EnvironmentBase):\n",
        "    def __init__(self, obstacle = [] ,id = 0, action_count=9, actionPrice = -1, goalReward = 100, punish=-10, j_limit = 15, i_limit = 15,\n",
        "                goal_state = np.array([0,0]), init_state = np.array([14,14]), p = 0.8, container=None):\n",
        "        \"\"\"\n",
        "        initialize your variables\n",
        "        \"\"\"\n",
        "        self.p = p\n",
        "        self.action_count = action_count\n",
        "        self.action_price = actionPrice\n",
        "        self.id = id\n",
        "        self.obstacles = obstacle\n",
        "        self.goal_reward = goalReward\n",
        "        self.punishment = punish\n",
        "        self.x_max = j_limit\n",
        "        self.y_max = i_limit\n",
        "        self.container = container\n",
        "        self.action_set = np.array([[1,1], [1,-1], [1,0], [0,1], [0,-1], [0,0], [-1,1], [-1,-1], [-1,0]])\n",
        "        self.goal_state = goal_state\n",
        "        self.state = init_state\n",
        "        \n",
        "    def isStatePossible(self, state):\n",
        "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
        "        if(state[0] < self.y_max and state[0] >= 0 and state[1] < self.x_max and state[1] >= 0 and state not in self.obstacles):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    def isAccessible(self, state, state_p):\n",
        "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
        "        accessible_states = state + self.action_set\n",
        "        if state_p in accessible_states:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
        "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
        "        if(self.isAccessible(state, state_p)):\n",
        "            if state+action == state_p:\n",
        "                return self.p\n",
        "            else:\n",
        "                return (1-self.p)/(self.action_count-1)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    \n",
        "    def getReward(self, state_p):\n",
        "        \"\"\"return reward of transition\"\"\"\n",
        "        total_reward = self.action_price\n",
        "        if (self.isStatePossible(state_p)):\n",
        "            total_reward += self.punishment\n",
        "        if state_p == self.goal_state:\n",
        "            total_reward += self.goal_reward\n",
        "        \n",
        "        return total_reward\n",
        "    \n",
        "    def sample_all_rewards(self):\n",
        "        return \n",
        "    \n",
        "    def calculate_reward(self, action):\n",
        "        ''' duplicate of get_reward '''\n",
        "\n",
        "        return \n",
        "\n",
        "\n",
        "    def terminated(self, state):\n",
        "        if state==state.goal_state:\n",
        "            return True \n",
        "\n",
        "    def observe(self):\n",
        "        return \n",
        "\n",
        "    def available_actions(self, state):\n",
        "        available_actions = np.array([])\n",
        "        for action in self.action_set:\n",
        "            temp_next_state = state + action\n",
        "            if self.isAccessible(temp_next_state):\n",
        "                available_actions = np.append(available_actions, action)\n",
        "        \n",
        "        return available_actions \n",
        "\n",
        "    def next_state(self, action):\n",
        "        return\n",
        "\n",
        "    def reset(self):\n",
        "        return\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        #print('{}:\\taction={}'.format(self.state['length'], self.state['last_action']))\n",
        "        return \n",
        "\n",
        "    def close(self):\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "898Jlhsycyes"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent(AgentBase):\n",
        "    def __init__(self, id, environment, discount, theta):\n",
        "        #initialize a random policy and V(s) = 0 for each state\n",
        "        self.environment = environment\n",
        "        \n",
        "        self.mapp = {}\n",
        "\n",
        "        self.q_values = np.zeros((self.environment.action_count, self.environment.x_max, self.environment.y_max))\n",
        "        \n",
        "        self.values = np.max(self.q_values, axis=0)\n",
        "        \n",
        "        self.policy = np.random.randint(low=0, high=self.environment.action_count, size=(self.environment.x_max, self.environment.y_max))\n",
        "        #init policy\n",
        "        super(Agent, self).__init__(id, environment)\n",
        "        self.discount = discount\n",
        "        self.theta = theta\n",
        "\n",
        "    def calc_q_value(self, action, state):\n",
        "        final_reward = 0\n",
        "        for s_prime in (state+self.actions):\n",
        "            prob = self.environment.getTransitionStatesAndProbs(state, action, s_prime)\n",
        "            reward = self.environment.getReward(s_prime)\n",
        "            final_reward += prob * (reward + self.discount * self.V[s_prime])\n",
        "        return final_reward\n",
        "        \n",
        "    def policy_evaluation(self):\n",
        "        pass\n",
        "    def policy_improvement(self):\n",
        "        pass\n",
        "    \n",
        "    def value_iteration(self):\n",
        "        pass\n",
        "    \n",
        "    def take_action(self) -> (object, float, bool, object):\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "env.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
